{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = clean_data(text)\n",
    "    \n",
    "    stp_words = list(set(stopwords.words('english')))\n",
    "    \n",
    "    return ' '.join([txt for txt in text.split(' ') if txt not in stp_words])\n",
    "def clean_data(text):\n",
    "    \n",
    "    return ' '.join([''.join(re.findall('([a-zA-Z0-9$!$])', txt)) for txt in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"This is a dataset BINARY for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more $$$ details.\"\n",
    "text_3 = clean_data(text_3)\n",
    "text_4 = text_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset BINARY for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text and already processed bag of words formats are provided See the README file contained in the release for more $$$ details'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset BINARY for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text and already processed bag of words formats are provided See the README file contained in the release for more $$$ details'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset BINARY for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text and already processed bag of words formats are provided See the README file contained in the release for more $$$ details This is a dataset BINARY for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text and already processed bag of words formats are provided See the README file contained in the release for more $$$ details'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([text_3, text_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_chars = lambda char: re.search('[#$?%%*&]', char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[$]', '$200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[#$?%%*&]', '$200')[0] in ['$', '#', '*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '0', '0', '4']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0-9*]', '200.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(re.findall('[#$?%!*&]', 'Hey!!!')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', '', '', '']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[#$?%!*&]', 'Hey!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = re.findall('[A-Za-z]', '200.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['a', 'b', 'c', 'a', 'c', 'a', 'a', 'b']\n",
    "# lst.remove('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'a', 'c', 'a', 'a', 'b']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 5, 6]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd = [i for i, d in enumerate(lst) if d == 'a']; cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n"
     ]
    }
   ],
   "source": [
    "if cd[0] in [0,1,2]:\n",
    "    print('T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c', 'c', 'a', 'b']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-201-d63ebf266d42>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-201-d63ebf266d42>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    lst.extend([[1 if not i == 1]])\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lst.extend([[1 if not i == 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-208-28c97b351f7f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-208-28c97b351f7f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [if not i == 1 i]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched\n"
     ]
    }
   ],
   "source": [
    "if not re.match('[A-Za-z]', '200.200'):\n",
    "    print(\"Matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cc\".isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cc)1="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.9"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(200.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '200']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[#$?%%*&]', '$200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', '']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[#$?%%*&]', 'hey?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EXL'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[0-9]', 'EXL4')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "spec_c = ['!', '#', '$', \"%\", '&', \"*\"]\n",
    "spec_c_dict = {\n",
    "            '!': \"EXL\",\n",
    "            '#': \"TAG\",\n",
    "            \"$\": \"CUR\",\n",
    "            \"?\": \"QUE\",\n",
    "            \"%\": \"PER\",\n",
    "            \"*\": \"CND\",\n",
    "            \"&\": \"AND\"\n",
    "        }\n",
    "for txt in text_3.split(' '):\n",
    "    if spec_chars(txt):\n",
    "        lst = list(txt)\n",
    "        spc_char = [ch for ch in lst if ch in spec_c]\n",
    "        spc_tot = Counter(spc_char)\n",
    "        if lst[0] == '$': arr.append(spec_c_dict['$']+str(spc_tot['$']))\n",
    "        if lst[0] == '#': \n",
    "            hstg = re.split('#', txt)[-1]\n",
    "            arr.append(spec_c_dict['#']+str(spc_tot['#'])+hstg)\n",
    "#                     _txt_l.append(hstg)\n",
    "        if lst[-1] == '?': arr.append(spec_c_dict['?']+str(spc_tot['?']))\n",
    "        if lst[-1] == '!': arr.append(spec_c_dict['!']+str(spc_tot['!']))\n",
    "    else:\n",
    "        arr.append(txt)\n",
    "        \n",
    "_dict = defaultdict(list)\n",
    "\n",
    "arr = list(set(arr))\n",
    "\n",
    "for index, data in enumerate(arr):\n",
    "    data = data.lower()\n",
    "    _dict[data].append(index)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary\n",
      "found\n"
     ]
    }
   ],
   "source": [
    "for key, value in _dict.items():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "#     print(type(value))\n",
    "    if idx in value:\n",
    "        print(key)\n",
    "        print('found')\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'provide': [0],\n",
       "             'a': [1],\n",
       "             'is': [2],\n",
       "             'highly': [3],\n",
       "             'use': [4],\n",
       "             'bag': [5],\n",
       "             'in': [6],\n",
       "             'sentiment': [7],\n",
       "             'file': [8],\n",
       "             'for': [9],\n",
       "             'cur3': [10],\n",
       "             '25000': [11],\n",
       "             'benchmark': [12],\n",
       "             'text': [13],\n",
       "             'and': [14],\n",
       "             'are': [15],\n",
       "             'as': [16],\n",
       "             'testing': [17],\n",
       "             'the': [18],\n",
       "             'set': [19],\n",
       "             'training': [20],\n",
       "             'readme': [21],\n",
       "             'data': [22],\n",
       "             'details': [23],\n",
       "             'movie': [24],\n",
       "             'this': [25],\n",
       "             'substantially': [26],\n",
       "             'datasets': [27],\n",
       "             'we': [28],\n",
       "             'there': [29],\n",
       "             'provided': [30],\n",
       "             'more': [31],\n",
       "             'of': [32],\n",
       "             'binary': [33, 50],\n",
       "             'contained': [34],\n",
       "             'words': [35],\n",
       "             'release': [36],\n",
       "             'well': [37],\n",
       "             'classification': [38],\n",
       "             'already': [39],\n",
       "             'processed': [40],\n",
       "             'see': [41],\n",
       "             'reviews': [42],\n",
       "             'additional': [43],\n",
       "             'formats': [44],\n",
       "             'dataset': [45],\n",
       "             'raw': [46],\n",
       "             'polar': [47],\n",
       "             'than': [48],\n",
       "             'previous': [49],\n",
       "             'unlabeled': [51],\n",
       "             'containing': [52]})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33, 50], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [51], [52]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-5dc3c53deb8e>, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-5dc3c53deb8e>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    return self.dict_[text][0] if text in list(self.dict_.keys())\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class tokenize():\n",
    "    \n",
    "    def __init__(self, text_corpus, remove_stop_words = False):\n",
    "        \n",
    "        self.text = text_corpus\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.stopwords = list(set(stopwords.words('english')))\n",
    "        self.spec_c = ['!', '@', '#', '$', '?']\n",
    "        self.spec_c_dict = {\n",
    "            '!': 'exl',\n",
    "            '@': 'eml',\n",
    "            '#': 'tag',\n",
    "            '$': 'cur',\n",
    "            '?': 'que'\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def clean_data(self, text):\n",
    "        \n",
    "        return ' '.join([''.join(re.findall('([a-zA-Z0-9$!#?])', txt)) for txt in text.split(' ')])\n",
    "        \n",
    "    def remove_stopwords(self, text):\n",
    "        \n",
    "        self.text = self.clean_data(text)\n",
    "        \n",
    "        return ' '.join([txt for txt in text.split(' ') if txt not in self.stopwords])\n",
    "    \n",
    "    def tagged_text(self):\n",
    "        \n",
    "        _txt_l = []\n",
    "        spec_chars = lambda char: re.search('[$#@!?]', char)\n",
    "        \n",
    "        if self.remove_stop_words:\n",
    "            self.text = self.remove_stopwords(self.clean_data(self.text))\n",
    "        else:\n",
    "            self.text = self.clean_data(self.text)\n",
    "        \n",
    "        for txt in self.text.split(' '):\n",
    "            if spec_chars(txt):\n",
    "#                 print(txt)\n",
    "                lst = list(txt)\n",
    "                spc_char = [ch for ch in lst if ch in spec_c]\n",
    "                spc_tot = Counter(spc_char)\n",
    "                if lst[0] == '$': _txt_l.append(self.spec_c_dict['$']+str(spc_tot['$']))\n",
    "                if lst[0] == '#': \n",
    "                    hstg = re.split('#', txt)[-1]\n",
    "                    _txt_l.append(self.spec_c_dict['#']+str(spc_tot['#'])+hstg)\n",
    "#                     _txt_l.append(hstg)\n",
    "                if lst[-1] == '?': _txt_l.append(self.spec_c_dict['?']+str(spc_tot['?']))\n",
    "                if lst[-1] == '!': _txt_l.append(self.spec_c_dict['!']+str(spc_tot['!']))\n",
    "            else:\n",
    "                _txt_l.append(txt)\n",
    "                \n",
    "        return ' '.join(_txt_l)\n",
    "    \n",
    "    \n",
    "    def strtoint(self, text):\n",
    "        if self.remove_stop_words:\n",
    "            self.text = self.remove_stopwords(self.clean_data(self.text))\n",
    "        else:\n",
    "            self.text = self.clean_data(self.text)\n",
    "            \n",
    "        self.tagged_text = self.tagged_text()\n",
    "        \n",
    "        self.dict_ = defaultdict(list)\n",
    "            \n",
    "        for index, data in enumerate(self.tagged_text.split(' ')):\n",
    "#             print(data)\n",
    "            \n",
    "            self.dict_[data].append(index)\n",
    "#         print(len(self.dict_))\n",
    "        return self.dict_[text][0] if text in list(self.dict_.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def strtoint(self, ind_text):\n",
    "        \n",
    "        if len(self.vocab_set) > 0:\n",
    "            if ind_text in self.vocab_set:\n",
    "                return self.vocab_set.index(ind_text)\n",
    "            else:\n",
    "                if self.special_ch(ind_text):\n",
    "                    if self.special_chars[re.findall('[#$?%%*&]', ind_text)[0]] in self.vocab_set:\n",
    "                        if re.findall('[!#$?%*&]', ind_text)[0] in ['$', '#', '*']:\n",
    "                            rt_1 = self.vocab_set.index(self.special_chars[re.findall('[#$?%%*&]', ind_text)[0]])\n",
    "                            txt = re.split(['!#$?%*&'], ind_text)[-1]\n",
    "                            if txt in self.vocab_set:\n",
    "                                rt_2 = self.vocab_set.index(txt)\n",
    "                            else:\n",
    "                                if not self.can_flt(txt):\n",
    "                                    rt_2 = self.vocab_set.index('UNKNUM')\n",
    "                                elif txt.isupper():\n",
    "                                    rt_2 = self.vocab_set.index('UNKCAPSTR')\n",
    "                                else:\n",
    "                                    rt_2 = self.vocab_set.index('UNKSTR')\n",
    "                            return [rt_1, rt_2]\n",
    "                        else:\n",
    "                            rt_1 = self.vocab_set.index(self.special_chars[list(set(re.findall('[#$?%!*&]', ind_text)))[0]])\n",
    "                            txt = re.split(['!#$?%*&'], ind_text)[0]\n",
    "                            if txt in self.vocab_set:\n",
    "                                rt_2 = self.vocab_set.index(txt)\n",
    "                            else:\n",
    "                                if not self.can_flt(txt):\n",
    "                                    rt_2 = self.vocab_set.index('UNKNUM')\n",
    "                                elif txt.isupper():\n",
    "                                    rt_2 = self.vocab_set.index('UNKCAPSTR')\n",
    "                                else:\n",
    "                                    rt_2 = self.vocab_set.index('UNKSTR')\n",
    "                            return [rt_1, rt_2]\n",
    "                    else:\n",
    "                        \n",
    "                        if re.findall('[!#$?%*&]', ind_text)[0] in ['$', '#', '*']:\n",
    "                            rt_1 = self.vocab_set.index(self.unk_chars[list(set(re.findall('[#$?%!*&]', ind_text)))[0]])\n",
    "                            txt = re.split('[[#$?%!*&]]', ind_text)[-1]      \n",
    "                            if txt in self.vocab_set:\n",
    "                                rt_2 = self.vocab_set.index(txt)\n",
    "                            else:\n",
    "                                if not self.can_flt(txt):\n",
    "                                    rt_2 = self.vocab_set.index('UNKNUM')\n",
    "                                elif txt.isupper():\n",
    "                                    rt_2 = self.vocab_set.index('UNKCAPSTR')\n",
    "                                else:\n",
    "                                    rt_2 = self.vocab_set.index('UNKSTR')\n",
    "                            return [rt_1, rt_2]\n",
    "                        else:\n",
    "                            rt_1 = self.vocab_set.index(self.unk_chars[list(set(re.findall('[#$?%!*&]', ind_text)))[0]])\n",
    "                            txt = re.split(['!#$?%*&'], ind_text)[0]\n",
    "                            if txt in self.vocab_set:\n",
    "                                rt_2 = self.vocab_set.index(txt)\n",
    "                            else:\n",
    "                                if not self.can_flt(txt):\n",
    "                                    rt_2 = self.vocab_set.index('UNKNUM')\n",
    "                                elif txt.isupper():\n",
    "                                    rt_2 = self.vocab_set.index('UNKCAPSTR')\n",
    "                                else:\n",
    "                                    rt_2 = self.vocab_set.index('UNKSTR')\n",
    "                            return [rt_1, rt_2]\n",
    "                else:\n",
    "                    if not self.can_flt(ind_text):\n",
    "                        return self.vocab_set.index('UNKNUM')\n",
    "                    elif ind_text.isupper():\n",
    "                        return self.vocan_set.index('UNKCAPSTR')\n",
    "                    else:\n",
    "                        return self.vocab_set.index('UNKSTR')\n",
    "                        \n",
    "        else:\n",
    "            return 'Please build a vocab first!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', '', '', '', '']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'text!!!!'\n",
    "re.split('[!#$?%*&]', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '$$$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Za-z0-9]', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = re.match('[A-Za-z0-9]', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if cc:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '$h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = lambda txt: re.search('([_a-zA-Z0-9_])', txt.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if cc(txt):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('([_a-zA-Z0-9_])', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search('([_a-zA-Z0-9_])', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vatsalsaglani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vatsalsaglani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed! bag of words formats are provided. See the README file contained in the release for more details.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = text = 'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text!!!! and already processed bag of words formats are provided. See the README file contained in the release for more details.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ThisisadatasetforbinarysentimentclassificationcontainingsubstantiallymoredatathanpreviousbenchmarkdatasetsWeprovideasetof25000highlypolarmoviereviewsfortrainingand25000fortestingThereisadditionalunlabeleddataforuseaswellRawtextandalreadyprocessedbagofwordsformatsareprovidedSeetheREADMEfilecontainedinthereleaseformoredetails'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(re.findall('[a-zA-Z0-9]', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more better way to remove the punctuations from the `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text and already processed! bag of words formats are provided See the README file contained in the release for more details'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([''.join(re.findall('([a-zA-Z0-9^!])', txt)) for txt in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text!!!! and already processed bag of words formats are provided See the README file contained in the release for more details'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([''.join(re.findall('([a-zA-Z0-9^!])', txt)) for txt in text_2.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_text = 'My name is Vatsal Saglani'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', ' ', ' ', ' ', 'V', ' ', 'S']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[^a-z]', name_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding `names` in a text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], ['Vatsal'], ['Saglani']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[re.findall('[^a-z].[a-z]+', txt) for txt in name_text.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vatsal Saglani'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([txt[0] for txt in [re.findall('[^a-z].[a-z]+', txt) for txt in name_text.split(' ')] if len(txt) > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding `numbers` in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_text = 'her phone number is (+91)-9790994455 and zip code is 560102'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`phonenuber` and `zipcodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9790994455', '560102']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num for num in re.findall('([+0-9]+)', number_text) if len(list(num)) >= 10 or len(list(num)) >= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9790994455']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d{10}', number_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d{6, 10}', number_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`phonenumber`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_text = 'her phone number is (+91) 979-099-4455 and zip code is 560102'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['979-099-4455']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d{3}-\\d{3}-\\d{4}', number_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9790994455'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([num for num in list(re.findall('\\d{3}-\\d{3}-\\d{4}', number_text)[0]) if not num == '-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of special characters\n",
    "\n",
    "- `!` - `unkexl` if `word` before not known else `exl{n}` where `n` equal to number of `!` after the word. Example, hey!!! `idx(`hey`)` `exl3`\n",
    "- `?` - `unkque` if `word` before not known else `que{n}` where `n` equal to numebr of `?` after the word. Exampel, what???? `idx(`what`)` `que4`\n",
    "- `$` - `unkcurr` -> _Start of a character_\n",
    "- `UPPERCASE` - `uprcs` -> _Upper case text_\n",
    "- `_%` - `unkper` -> % symbol in a character\n",
    "- `#` - `tag` if text after hashtag known else `unktag` if text after hashtag unknown.\n",
    "\n",
    "_*add extra when needed_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join([''.join(re.findall('([a-zA-Z0-9^!])', txt)) for txt in text_2.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    \n",
    "    return ' '.join([''.join(re.findall('([a-zA-Z0-9!#$?%*&])', txt)) for txt in text.strip().split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text!!!! and already processed bag of words formats are provided See the README file contained in the release for more details'"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more $$$ details.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw text and already processed bag of words formats are provided See the README file contained in the release for more $$$ details'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = clean_data(text)\n",
    "    \n",
    "    stp_words = list(set(stopwords.words('english')))\n",
    "    \n",
    "    return ' '.join([txt for txt in text.split(' ') if txt not in stp_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This dataset binary sentiment classification containing substantially data previous benchmark datasets We provide set 25000 highly polar movie reviews training 25000 testing There additional unlabeled data use well Raw text!!!! already processed bag words formats provided See README file contained release details'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This dataset binary sentiment classification containing substantially data previous benchmark datasets We provide set 25000 highly polar movie reviews training 25000 testing There additional unlabeled data use well Raw text already processed bag words formats provided See README file contained release $$$ details'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 'hey !! red'\n",
    "cc = re.search('[$%!#@]', tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if cc:\n",
    "    print(\"True\")\n",
    "else: \n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    " spec_chars = lambda char: re.search('[$&#@!?]', char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = spec_chars(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 'hey!!!$$$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(\"[!#@$%]\", tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 'hey!!'\n",
    "spec_c = ['!', '@', '#', '$', '@', '?']\n",
    "lst = list(tt)\n",
    "spc_char = [ch for ch in lst if ch in spec_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '!']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spc_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'heydude']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = '#heydude'\n",
    "re.split('#', tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize():\n",
    "    \n",
    "    def __init__(self, text_corpus, remove_stop_words = False):\n",
    "        \n",
    "        self.remove = remove_stop_words\n",
    "        self.special_chars = {\n",
    "            '!': \"EXL\",\n",
    "            '#': \"TAG\",\n",
    "            \"$\": \"CUR\",\n",
    "            \"?\": \"QUE\",\n",
    "            \"%\": \"PER\",\n",
    "            \"*\": \"CND\",\n",
    "            \"&\": \"AND\"\n",
    "        }\n",
    "        self.unk_chars = {\n",
    "            '!': \"UNKEXL\",\n",
    "            '#': \"UNKTAG\",\n",
    "            \"$\": \"UNKCUR\",\n",
    "            \"?\": \"UNKQUE\",\n",
    "            \"%\": \"UNKPER\",\n",
    "            \"*\": \"UNKCND\",\n",
    "            \"&\": \"UNKAND\"\n",
    "        }\n",
    "        \n",
    "        self.special_c = ['!', '#', '$', '?', '%', '*', '&']\n",
    "        self.can_flt = lambda char: re.match('[A-Za-z]', char)\n",
    "        self.text_corpus = text_corpus\n",
    "        self.is_txt = lambda txt: re.search('([_a-zA-Z0-9_])', txt.strip())\n",
    "#         self.vocab_set = []\n",
    "        \n",
    "        if self.remove:\n",
    "            self.stopwords = list(set(stopwords.words('english')))\n",
    "        \n",
    "        self.build_vocab(self.text_corpus)\n",
    "            \n",
    "        \n",
    "            \n",
    "    def clean_data(self, text):\n",
    "        \n",
    "        return ' '.join([''.join(re.findall('([a-zA-Z0-9!#$?!%*&])', txt)) for txt in text.strip().split(' ')])\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \n",
    "        self.text = self.clean_data(text)\n",
    "        \n",
    "        return ' '.join([txt for txt in text.split(' ') if txt not in self.stopwords])\n",
    "    \n",
    "    def build_vocab(self, text_corpus):\n",
    "        \n",
    "        self.vocab_set = [] # for str_to_int\n",
    "        self.vocab_dict = dict()\n",
    "        \n",
    "        if self.remove:\n",
    "            text_corpus = self.remove_stopwords(self.clean_data(text_corpus))\n",
    "        else:\n",
    "            text_corpus = self.clean_data(text_corpus)\n",
    "        \n",
    "        self.special_ch = lambda char: re.search('[#$?!%*&]', char)\n",
    "        \n",
    "        for text in text_corpus.split(' '):\n",
    "            if text not in self.vocab_set:\n",
    "                if self.special_ch(text):\n",
    "                    lst = list(text)\n",
    "                    spc_char = [ch for ch in lst if ch in self.special_c]\n",
    "                    if lst[0] == '$': self.vocab_set.extend([self.special_chars['$'] if self.special_chars['$'] not in self.vocab_set else '-1233', re.split('[$]', text)[-1] if re.split('[$]', text)[-1] not in self.vocab_set else '-1233']) \n",
    "                    if lst[0] == '?': self.vocab_set.extend([self.special_chars['?'] if self.special_chars['?'] not in self.vocab_set else '-1233', re.split('[?]', text)[-1] if re.split('[?]', text)[-1] not in self.vocab_set else '-1233']) \n",
    "                    if lst[0] == '!': self.vocab_set.extend([self.special_chars['!'] if self.special_chars['!'] not in self.vocab_set else '-1233', re.split('[!]', text)[-1] if re.split('[!]', text)[-1] not in self.vocab_set else '-1233']) \n",
    "                    if lst[0] == '%': self.vocab_set.extend([self.special_chars['%'] if self.special_chars['%'] not in self.vocab_set else '-1233', re.split('[%]', text)[-1] if re.split('[%]', text)[-1] not in self.vocab_set else '-1233']) \n",
    "                    if text == '&' and 'AND' not in self.vocab_set: self.vocab_set.append(self.special_chars['&']) \n",
    "                    if lst[0] == '*': self.vocab_set.extend([self.special_chars['*'] if self.special_chars['*'] not in self.vocab_set else '-1233', re.split('[*]', text)[-1] if re.split('[*]', text)[-1] not in self.vocab_set else '-1233']) \n",
    "                    if lst[0] == '#': self.vocab_set.extend([self.special_chars['#'] if self.special_chars['#'] not in self.vocab_set else '-1233', re.split('[#]', text)[-1] if re.split('[#]', text)[-1] not in self.vocab_set else '-1233']) \n",
    "                else:\n",
    "                    self.vocab_set.append(text)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        if '-1233' in self.vocab_set:\n",
    "            self.vocab_set = [vocab for vocab in self.vocab_set if not vocab == '-1233']\n",
    "        \n",
    "        unk_lst = []\n",
    "        \n",
    "        for knw in list(self.special_chars.values()):\n",
    "            if knw not in self.vocab_set:\n",
    "                ky = [key for key, value in self.special_chars.items() if value == knw][0]\n",
    "                unk_lst.append(self.unk_chars[ky])\n",
    "        \n",
    "        self.vocab_set.extend(unk_lst)\n",
    "        self.vocab_set.extend(['UNKNUM', 'UNKCAPSTR', 'UNKSTR'])\n",
    "            \n",
    "            \n",
    "         \n",
    "        self.vocab_dict = defaultdict(list)\n",
    "        \n",
    "        for index, data in enumerate(self.vocab_set):\n",
    "            data = data.lower() if isinstance(data, str) else data\n",
    "            self.vocab_dict[data].append(index)\n",
    "            \n",
    "        return self.vocab_set, self.vocab_dict\n",
    "    \n",
    "    def get_vocab_len(self):\n",
    "        if len(self.vocab_set) > 0:\n",
    "            return len(self.vocab_set)\n",
    "        else:\n",
    "            return \"Build a vocab first\"\n",
    "    \n",
    "    def get_tagged_sentence(self, text):\n",
    "        \n",
    "        if self.remove:\n",
    "            text = self.remove_stopwords(self.clean_data(text))\n",
    "        else:\n",
    "            text = self.clean_data(text)\n",
    "            \n",
    "        text_lst = text.split(' ')\n",
    "        if len(self.vocab_set) > 0:\n",
    "            join_lst = []\n",
    "            for txt in text_lst:\n",
    "                if txt in self.vocab_set:\n",
    "                    join_lst.append(txt)     \n",
    "                else:\n",
    "                    # not in vocab set\n",
    "                    if self.special_ch(txt):\n",
    "                        spc_chr = self.special_chars[re.findall('[#$?!%*&]', txt)[0]]\n",
    "                        if spc_chr in self.vocab_set:\n",
    "                            # special char in vocab\n",
    "                            join_lst.append(spc_chr)\n",
    "                            if self.is_txt(txt) and re.findall('[#$?!%*&]', txt)[0] in ['#', '$', '*']:\n",
    "                                \n",
    "                                t = re.split(['!#$?%*&'], txt)[-1]\n",
    "                                if t in self.vocab_set:\n",
    "                                    join_lst.append(t)\n",
    "                                else:\n",
    "                                    if not self.can_flt(t):\n",
    "                                        join_lst.append('UNKNUM')\n",
    "                                    elif t.isupper():\n",
    "                                        join_lst.append('UNKCAPSTR')\n",
    "                                    else:\n",
    "                                        join_lst.append('UNKSTR')\n",
    "                            else:\n",
    "#                                 print(txt)\n",
    "                                if self.is_txt(txt):\n",
    "                                    t = re.split('[!#$?%*&]', txt)[0]\n",
    "                                    if t in self.vocab_set:\n",
    "                                        join_lst.append(t)\n",
    "                                    else:\n",
    "                                        if not self.can_flt(t):\n",
    "                                            join_lst.append('UNKNUM')\n",
    "                                        elif t.isupper():\n",
    "                                            join_lst.append('UNKCAPSTR')\n",
    "                                        else:\n",
    "                                            join_lst.append('UNKSTR')\n",
    "                        else:\n",
    "                            # special char not in voacb\n",
    "                            chr_ = re.findall('[#$?!%*&]', txt)[0]\n",
    "                            join_lst.append(self.unk_chars[chr_])\n",
    "                            if self.is_txt(txt) and chr_ in ['#', '$', '*']:\n",
    "                                t = re.split('[!#$?%*&]', txt)[-1]\n",
    "                                if t in self.vocab_set:\n",
    "                                    join_lst.append(t)\n",
    "                                else:\n",
    "                                    if not self.can_flt(t):\n",
    "                                        join_lst.append('UNKNUM')\n",
    "                                    elif t.isupper():\n",
    "                                        join_lst.append('UNKCAPSTR')\n",
    "                                    else:\n",
    "                                        join_lst.append('UNKSTR')\n",
    "                            else:\n",
    "                                if self.is_txt(txt):\n",
    "                                    t = re.split('[!#$?%*&]', txt)[0]\n",
    "                                    if t in self.vocab_set:\n",
    "                                        join_lst.append(t)\n",
    "                                    else:\n",
    "                                        if not self.can_flt(t):\n",
    "                                            join_lst.append('UNKNUM')\n",
    "                                        elif t.isupper():\n",
    "                                            join_lst.append('UNKCAPSTR')\n",
    "                                        else:\n",
    "                                            join_lst.append('UNKSTR')                         \n",
    "                    else:\n",
    "                        # not special char\n",
    "                        if not self.can_flt(txt):\n",
    "                            join_lst.append('UNKNUM')\n",
    "                        elif txt.isupper():\n",
    "                            join_lst.append('UNKCAPSTR')\n",
    "                        else:\n",
    "                            join_lst.append('UNKSTR')\n",
    "            tagged_text = ' '.join(join_lst)\n",
    "            return tagged_text\n",
    "                            \n",
    "        else:\n",
    "            return \"Build a vocab first\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_indices(self, text):\n",
    "        if self.remove:\n",
    "            text = self.remove_stopwords(self.clean_data(text))\n",
    "        else:\n",
    "            text = self.clean_data(text)\n",
    "            \n",
    "#          if self.remove:\n",
    "                \n",
    "#              text = self.remove_stopwords(self.clean_data(text))\n",
    "#         else:\n",
    "#             text = self.clean_data(text)\n",
    "            \n",
    "        tagged_text = self.get_tagged_sentence(text)\n",
    "        lst = []\n",
    "        for txt in tagged_text.strip().split(' '):\n",
    "            lst.append(self.vocab_set.index(txt))\n",
    "        \n",
    "        return np.array(lst)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    def from_df(self, dataframe, columname):\n",
    "        \n",
    "        join_lst = dataframe[columname].values.tolist()\n",
    "        \n",
    "        string = ' '.join(join_lst)\n",
    "        \n",
    "        self.build_vocab(string)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = \"He’s making his list and checking it twice. #Batman80 #LongLiveTheBat  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He’s making his list and checking it twice. #Batman80 #LongLiveTheBat  '"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hes making his list and checking it twice #Batman80 #LongLiveTheBat'"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He’s making his list and checking it twice. #Batman80 #LongLiveTheBat'"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hes making his list and checking it twice #Batman80 #LongLiveTheBat'"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hes making his list and checking it twice #Batman80 #LongLiveTheBat'"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenize(tweet_text).clean_data(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Tokenize(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hes',\n",
       " 'making',\n",
       " 'his',\n",
       " 'list',\n",
       " 'and',\n",
       " 'checking',\n",
       " 'it',\n",
       " 'twice',\n",
       " 'TAG',\n",
       " 'Batman80',\n",
       " 'LongLiveTheBat',\n",
       " 'UNKEXL',\n",
       " 'UNKCUR',\n",
       " 'UNKQUE',\n",
       " 'UNKPER',\n",
       " 'UNKCND',\n",
       " 'UNKAND',\n",
       " 'UNKNUM',\n",
       " 'UNKCAPSTR',\n",
       " 'UNKSTR']"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'hes': [0],\n",
       "             'making': [1],\n",
       "             'his': [2],\n",
       "             'list': [3],\n",
       "             'and': [4],\n",
       "             'checking': [5],\n",
       "             'it': [6],\n",
       "             'twice': [7],\n",
       "             'tag': [8],\n",
       "             'batman80': [9],\n",
       "             'longlivethebat': [10],\n",
       "             'unkexl': [11],\n",
       "             'unkcur': [12],\n",
       "             'unkque': [13],\n",
       "             'unkper': [14],\n",
       "             'unkcnd': [15],\n",
       "             'unkand': [16],\n",
       "             'unknum': [17],\n",
       "             'unkcapstr': [18],\n",
       "             'unkstr': [19]})"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = text_3 + ' !!' + ' ?' + ' %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more $$$ details. BINARY !! ? %'"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Tokenize(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets We provide a set of 25000 highly polar movie reviews for training and 25000 for testing There is additional unlabeled data for use as well Raw UNKSTR and already processed bag of words formats are provided See the README file contained in the release for more UNKCUR details UNKCAPSTR UNKEXL UNKQUE UNKPER'"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.get_tagged_sentence(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17,  2, 18, 19, 20, 21, 22, 23, 24,  4, 25, 26, 20,  4, 27, 28,  1,\n",
       "       29, 30, 11,  4, 31, 32, 33, 34, 59, 26, 35, 36, 37, 19, 38, 39, 40,\n",
       "       41, 42, 43, 44, 45, 46, 47, 43, 48,  4, 10, 52, 49, 58, 50, 53, 54])"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.get_indices(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'dataset',\n",
       " 'for',\n",
       " 'binary',\n",
       " 'sentiment',\n",
       " 'classification',\n",
       " 'containing',\n",
       " 'substantially',\n",
       " 'more',\n",
       " 'data',\n",
       " 'than',\n",
       " 'previous',\n",
       " 'benchmark',\n",
       " 'datasets',\n",
       " 'We',\n",
       " 'provide',\n",
       " 'set',\n",
       " 'of',\n",
       " '25000',\n",
       " 'highly',\n",
       " 'polar',\n",
       " 'movie',\n",
       " 'reviews',\n",
       " 'training',\n",
       " 'and',\n",
       " 'testing',\n",
       " 'There',\n",
       " 'additional',\n",
       " 'unlabeled',\n",
       " 'use',\n",
       " 'as',\n",
       " 'well',\n",
       " 'Raw',\n",
       " 'text',\n",
       " 'already',\n",
       " 'processed',\n",
       " 'bag',\n",
       " 'words',\n",
       " 'formats',\n",
       " 'are',\n",
       " 'provided',\n",
       " 'See',\n",
       " 'the',\n",
       " 'README',\n",
       " 'file',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'release',\n",
       " 'CUR',\n",
       " '',\n",
       " 'details',\n",
       " 'BINARY',\n",
       " 'EXL',\n",
       " 'QUE',\n",
       " 'PER',\n",
       " 'UNKTAG',\n",
       " 'UNKCND',\n",
       " 'UNKAND',\n",
       " 'UNKNUM',\n",
       " 'UNKCAPSTR',\n",
       " 'UNKSTR']"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
